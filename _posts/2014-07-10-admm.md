---
layout: page
title: "ADMM"
category: ref
date: 2014-07-10 21:27:50
---

This is a high level description of the [Alternating Direction Method of Multipliers][admm_distr_stats] (ADMM) specific to graph form problems. For more detail we recommmend reading reading the papers in the references section.


### Augmented Lagrangian

Consider the convex optimiztion problem

\\begin{equation}
	\\begin{aligned}
    &\\text{minimize}
    & & f(x) \\\\\\
    & \\text{subject to} 
    & & x \\in \\mathcal{C},
	\\end{aligned} ~~~~~~~~~~\\text{[1]}
\\end{equation}

where \\( f \\) is a convex function and \\( \\mathcal{C} \\) is a convex set. This is clearly equivalent to 

\\[
	\\begin{aligned}
    &\\text{minimize}
    & & f(x) + I(z \\in \\mathcal{C}) \\\\\\
    & \\text{subject to} 
    & & x = z.
	\\end{aligned}
\\]

which again is equivalent to the augmented problem

\\[
	\\begin{aligned}
    &\\text{minimize}
    & & f(x) + I(z \\in \\mathcal{C}) + (\\rho/2) \\|x - z\\|^2 \\\\\\
    & \\text{subject to} 
    & & x = z.
	\\end{aligned}
\\]

We can write this using Lagrange multipliers

\\[
  \\inf\_{x,z} \\sup\_y~~f(x) + I(z \\in \\mathcal{C}) + y^T(x - z) + (\\rho/2) \\|x - z\\|^2
\\]

The function \\( L\_\\rho(x,y,z) = f(x) + I(z \\in \\mathcal{C}) + y^T(x - z) + (\\rho/2) \\|x - z\\|^2 \\) is known as the _augmented Lagrangian_ of [1].

### Alternating Direction Method of Multipliers

Consider the problem of finding \\( x^\\star, y^\\star, z^\\star \\) such that

\\[
  L\_\\rho(x^\\star, y^\\star, z^\\star) = \\inf\_{x,z} \\sup\_y L\_\\rho(x, y, z)
\\]

Solving this problem is equivalent to solving the original problem and is therefore just as hard. ADMM consists of alternating between minimizing \\( L\_\\rho \\) over \\( x \\), then \\( z \\) and lastly taking a gradient step with respect to \\( y \\). The algorithm can be written as 

\\[
  \\begin{aligned}
     x^{k+1} &=\\arg\\min\_x~L\_\\rho(x, y^k, z^k) \\\\\\
     z^{k+1} &=\\arg\\min\_x~L\_\\rho(x^{k+1}, y^k, z) \\\\\\
     y^{k+1} &= y^k + \\rho (x^{k+1} - z^{k+1})
	\\end{aligned}
\\]

_Note_, this is a slightly simplified version of ADMM.

### ADMM applied to Graph Form Problems

To apply ADMM to graph form problems use the equivalence table

\\[
  \\begin{aligned}
    &\\text{Problem [1]} && \\text\{Graph Form\} \\\\\\
    &x                   && (x, y) \\\\\\
    &f                   && f + g  \\\\\\
    &\\mathcal{C}        && \\{ (x, y) ~\|~ y = Ax \\}
  \\end{aligned}
\\]


### Pros and Cons of using ADMM

A couple of reasons why ADMM can be much faster than conventional methods
 
  + Alternating minimization makes it possible to perform much of the work in parallel.
  + By using factorization caching, a large portion of the work can be re-used between iterations.

Some considerations when using ADMM

  + The convergence rate is highly dependent on the choice of \\( \\rho \\).
  + Convergence can be slow if you require more than 3-4 digits of accuracy (if you do, then you should be quite confident that your model is correct and that your data is accurate the more than 3-4 digits of precision).


### References
1. [Block Splitting for Distributed Optimization -- N. Parikh and S. Boyd][block_splitting]
2. [Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers -- S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein][admm_distr_stats]
3. [Proximal Algorithms -- N. Parikh and S. Boyd][prox_algs]

[block_splitting]: http://www.stanford.edu/~boyd/papers/block_splitting.html "Block Splitting for Distributed Optimization -- N. Parikh and S. Boyd"

[admm_distr_stats]: http://www.stanford.edu/~boyd/papers/block_splitting.html "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers -- S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein"

[prox_algs]: http://www.stanford.edu/~boyd/papers/prox_algs.html "Proximal Algorithms -- N. Parikh and S. Boyd"
