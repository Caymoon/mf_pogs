{"name":"Pogs","tagline":"Proximal Operator Graph Solver","body":"POGS\r\n====\r\n\r\nPOGS is a solver for convex optimization problems in _graph form_ using [Alternating Direction Method of Multipliers][admm_distr_stats] (ADMM). The algorithm is described in [this paper][block_splitting]. \r\n\r\n----\r\nA graph form problem can be expressed as\r\n\r\n```\r\nminimize        f(y) + g(x)\r\nsubject to      y = Ax\r\n```\r\nwhere `f` and `g` are convex and can take on the values `R \\cup {âˆž}`. The solver requires that the [proximal operators][prox_algs] of `f` and `g` are known and that `f` and `g` are separable, meaning that they can be written as\r\n\r\n```\r\nf(y) = sum_{i=1}^m f_i(y_i)\r\ng(x) = sum_{i=1}^n g_i(x_i)\r\n```\r\n\r\nThe following functions are currently supported\r\n\r\n  + `f(x) = |x|`\r\n  + `f(x) = x log(x)`\r\n  + `f(x) = e^x`\r\n  + `f(x) = huber(x)`\r\n  + `f(x) = x`\r\n  + `f(x) = I(0 <= x <= 1)`\r\n  + `f(x) = I(x = 0)`\r\n  + `f(x) = I(x >= 0)`\r\n  + `f(x) = I(x <= 0)`\r\n  + `f(x) = log(1 + e^x)`\r\n  + `f(x) = max(0, -x)`\r\n  + `f(x) = max(0, x)`\r\n  + `f(x) = -log(x)`\r\n  + `f(x) = 1/x`\r\n  + `f(x) = (1/2) x^2`\r\n  + `f(x) = 0`\r\n\r\nwhere `I(.)` is the indicator function, taking on the value 0 if the condition is satisfied and infinity otherwise. More functions can be added by modifying the proximal operator header file: `<pogs>/src/prox_lib.h`.\r\n\r\n\r\nLanguages / Frameworks\r\n======================\r\nThree different implementations of the solver are either planned or already supported:\r\n\r\n  1. MATLAB: A MATLAB implementation along with examples can be found in the `<pogs>/matlab` directory.\r\n  2. C++/BLAS/OpenMP: A CPU version can be found in the file `<pogs>/src/pogs.cpp`. POGS must be linked to a BLAS library (such as the Apple Accelerate Framework or ATLAS).\r\n  3. C++/cuBLAS/CUDA: The GPU version is located in the file `<pogs>/src/pogs.cu`.\r\n\r\n\r\nProblem Classes\r\n===============\r\n\r\nAmong others, the solver can be used for the following classes of (linearly constrained) problems\r\n\r\n  + Least Squares\r\n  + Lasso, Ridge Regression, Logistic Regression, Huber Fitting and Elastic Net Regulariation \r\n  + Linear Programs and Quadratic Programs\r\n  + Analytic Centering\r\n\r\n\r\nReferences\r\n==========\r\n1. [Block Splitting for Distributed Optimization -- N. Parikh and S. Boyd][block_splitting]\r\n2. [Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers -- S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein][admm_distr_stats]\r\n3. [Proximal Algorithms -- N. Parikh and S. Boyd][prox_algs]\r\n\r\n\r\n[block_splitting]: http://www.stanford.edu/~boyd/papers/block_splitting.html \"Block Splitting for Distributed Optimization -- N. Parikh and S. Boyd\"\r\n\r\n[admm_distr_stats]: http://www.stanford.edu/~boyd/papers/block_splitting.html \"Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers -- S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein\"\r\n\r\n[prox_algs]: http://www.stanford.edu/~boyd/papers/prox_algs.html \"Proximal Algorithms -- N. Parikh and S. Boyd\"\r\n\r\n\r\nAuthor\r\n------\r\nChris Fougner (fougner@stanford.edu)\r\n\r\nAcknowledgement: Much of this is based on work by Neal Parikh.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}